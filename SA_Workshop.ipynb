{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad4a401",
   "metadata": {},
   "source": [
    "<img src=\".\\CP_Logo.PNG\" align=\"left\" height=\"380\" width=\"320\" style=\"padding-right;5px\">\n",
    "\n",
    "# Sentiment Analysis in Marketing\n",
    "---\n",
    "\n",
    "Sentiment analysis (also known as opinion mining) is the use of Natural Language Processing (NLP), textual analysis, and other computational linguistic techniques to identify and categorize the author's attitude towards a particular topic, product, etc. as either positive, negative, or neutral. In terms of marketing, sentiment analysis provides a method to qualify textual responses to advertisement campaigns, product anouncements, and social media posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62f6d8",
   "metadata": {},
   "source": [
    "## Required Imports:\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "<font style=\"color:darkred\"><b>Note:</b>If you have not previously installed these `packages`, you can use the cell below to perform the required `pip` installs.</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c41040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you still need to perform some pip installs:\n",
    "! pip install --user pandas -q\n",
    "! pip install --user nltk -q\n",
    "! pip install --user wordcloud -q\n",
    "! pip install --user tensorflow -q\n",
    "! pip install --user transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20092331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2f80e",
   "metadata": {},
   "source": [
    "## Our Data\n",
    "---\n",
    "\n",
    "In 2018, Nike announced a partnership with Colin Kaepernick as the face for the 30th anniversary of their JustDoIt campaign along with the slogan, \"Believe in something, even if it means sacrificing everything.\"\n",
    "\n",
    "This campaign prompted a lot of controversy, which subsequently yielded a polarized social media response to the campaign. A snapshot of 5,000 tweets containing the #JustDoIt hashtag was captured days after the campaign launched, on September 7th, 2018.\n",
    "\n",
    "We will be examining the `tweet_full_text` field in the dataset to assess the sentiment of the user's text.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "This dataset is hosted on the popular Data Science competition and collaboration site, Kaggle:\n",
    "\n",
    "https://www.kaggle.com/datasets/eliasdabbas/5000-justdoit-tweets-dataset\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dataframe by reading in the csv\n",
    "df = pd.read_csv('justdoit_tweets.csv')\n",
    "# Simplify the dataframe based on the 4 columns listed\n",
    "df = df[['tweet_created_at', 'tweet_full_text', 'user_screen_name', 'user_location']]\n",
    "# Quickly impute missing values with a string of unknown\n",
    "df.loc[(df.user_location.isna()), \"user_location\"] = 'Unknown'\n",
    "df.loc[(df.user_screen_name.isna()), \"user_screen_name\"] = 'Unknown'\n",
    "# Convert the date column to a datetime dtype\n",
    "df['tweet_created_at'] = pd.to_datetime(df['tweet_created_at'], errors ='ignore')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9cab1",
   "metadata": {},
   "source": [
    "# From the Top\n",
    "---\n",
    "\n",
    "Firstly, as we have seen with our ad campaign dataset, our textual samples do not readily come with a label to represent sentiment. In order to start building our sentiment analysis model, we will need to use a dataset that contains those labels for the training of our model. One of the most popular beginning datasets is the IMDB reviews dataset, which can be pulled directly from Tensorflow datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDB Reviews dataset\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b03f88",
   "metadata": {},
   "source": [
    "## Textual Preprocessing\n",
    "---\n",
    "\n",
    "One of the more difficult tasks associated with any textual analysis is performing the required preprocessing techniques to transform the raw text into an analyzable and interpretable format for machine learning algorithms. We will take a look at some basic preprocessing methodologies below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train and test sets\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "# Initialize sentences and labels lists\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# Loop over all training examples and save the sentences and labels\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(s.numpy().decode('utf8'))\n",
    "    training_labels.append(l.numpy())\n",
    "\n",
    "# Loop over all test examples and save the sentences and labels\n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(s.numpy().decode('utf8'))\n",
    "    testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35366879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8744ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a stopword list based on stopwords from two different sources\n",
    "stopword_list = list(set(stopwords.words(\"english\") + list(STOPWORDS)))\n",
    "stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to remove the stopwords, we iterate over the length of the training samples\n",
    "for i in range(len(training_sentences)):\n",
    "    # Tokenize the words of each sample as we loop\n",
    "    word_tokens = word_tokenize(training_sentences[i])\n",
    "    # Filter the sample to remove stopwords\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stopword_list]\n",
    "    # Convert from a list of words back to a string\n",
    "    s = str(filtered_sentence)\n",
    "    # Remove punctuation based on regex\n",
    "    s = re.sub(r'[^\\w\\s]','', s)\n",
    "    # Remove any double spaces created as a result of our processes\n",
    "    s = s.replace(\"  \", ' ')\n",
    "    # Replace the original sample with our processed version\n",
    "    training_sentences[i] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any new samples introduced must undergo the same preprocessing, so repeat for the testing/validation samples\n",
    "for i in range(len(testing_sentences)):\n",
    "    word_tokens = word_tokenize(testing_sentences[i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stopword_list]\n",
    "    s = str(filtered_sentence)\n",
    "    s = re.sub(r'[^\\w\\s]','', s)\n",
    "    s = s.replace(\"  \", ' ')\n",
    "    testing_sentences[i] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75343b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Review the changes by viewing the first sample in training\n",
    "training_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb12272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will tokenize the samples for conversion to sequences\n",
    "tokenizer = Tokenizer(num_words = 10000, # Size of the vocabulary we want our model to have\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', # In case we have any remaining punctuation, etc.\n",
    "                      lower=True, # Sets all samples to lowercase\n",
    "                      oov_token = \"<OOV>\") # In case there is a word outside of our vocabulary present in the samples\n",
    "\n",
    "# We only fit the tokenizer to the training portion of the data\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen = 120, truncating = 'post')\n",
    "\n",
    "# Generate and pad the test sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels lists to numpy array\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed6977",
   "metadata": {},
   "source": [
    "## Build a Model\n",
    "---\n",
    "\n",
    "The Sequential model is a linear stack of layers, which can take a singular input of data and generate a singular output as a response. There are various different types of layers that can be used and your layer schema usually depends on the type of problem you are trying to solve. For the purposes of this demo, we will be using 3 different layer types: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d507f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    # Embedding layer enables us to convert each word into a fixed length vector of defined size.\n",
    "    tf.keras.layers.Embedding(10000, 64), # Size of vocabulary & output dim length of vector for each word\n",
    "    # RNN architecture for sequential prediction, past to present & present to past\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    # Fully connected dense layer, transform summed weighted input with ReLu activation\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    # Fully connected dense layer, logistic activation with binary output (postive or negative)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training parameters and optimzation\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208dac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(padded, training_labels_final, epochs=2, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a9eba",
   "metadata": {},
   "source": [
    "## Export Model\n",
    "---\n",
    "\n",
    "In many cases, you will build and train a model for the purposes of making future predictions. An easy method to accomplish this is to structure an `export_model`, which includes our trained model with the appropriate activation (i.e. `sigmoid`). This `export_model` will be compiled the same as our original model, but we won't have to fit it to any new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148654ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the trained model in our new export_model schema and use the appropriate activation\n",
    "export_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "# Use the same compiling parameters as before\n",
    "export_model.compile(\n",
    "    loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a list of new samples\n",
    "examples = [\n",
    "  \"The show was great! I enjoyed the special effects and the acting was fantastic. I would watch this movie again and again!\",\n",
    "  \"The movie was terrible, horrible acting and special effects. I couldn't follow the story and just wanted it to end.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea950aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to apply the same preprocessing\n",
    "for i in range(len(examples)):\n",
    "    word_tokens = word_tokenize(examples[i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stopword_list]\n",
    "    s = str(filtered_sentence)\n",
    "    s = re.sub(r'[^\\w\\s]','', s)\n",
    "    s = s.replace(\"  \", ' ')\n",
    "    examples[i] = s\n",
    "\n",
    "# Generate and pad the new sequences\n",
    "example_sequences = tokenizer.texts_to_sequences(examples)\n",
    "example_padded = pad_sequences(example_sequences, maxlen = 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions based on the new samples\n",
    "export_model.predict(example_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1fa24",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Congratulations, you have successfully preprocessed textual samples and trained a model to determine author sentiment. Now that you have an exported model, analyze the sentiment of the Nike ad campaign tweets and inspect your model's performance!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
